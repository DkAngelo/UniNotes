 > *Lezione del 10/03/2025*

Problema della classificazione: dato un vettore di elementi che descrive un elemento, vorrei indovinare la sua classe. Dal punto di vista del learning, abbiamo un dataset per la quale voglio imparare una funzione in grado di predire la classe dell'elemento.
L'errore è il numero di volte che la nostra funziona non indovina la classe che vogliamo prevedere.
L'accuratezza è l'esatto opposto, quindi il numero di volte in cui la prevede rispetto ai tentativi totali.
Da un punto di vista probabilistico dobbiamo introdurre un paio di elementi:
- $P\left(Y=c\right)$ si chiama probabilità a priori ed è riferita alla classe nel suo insieme.
- $P\left(X=x\vert Y=c\right)$ si chiama likelihood e ci dice quanto l'elemento x sia compatibile con la classe c (quanto l'elemento x assomigli agli elementi di c).
La probabilità della classe c osservato l'elemento x (probabilità a posteriori) si calcola con la regola di Bayes  $P\left(Y=c\vert X=x\right)$:
![](Images/Pasted%20image%2020250310111938.png)

Dato un dataset posso scegliere due strategie:
- **maximum likelihood**
	- abbiamo una likelihood per ogni classe presente
	- bloccata la classe, gli do un elemento e mi dice quanto questo sia compatibile con la classe data
	- scelgo la classe per cui la likelihood rispetto all'elemento è la più alta
- **maximum a posteriori**
	- abbiamo una likelihood e una prior probability per ogni classe
	- dato un nuovo elemento, applico la regola di bayes per l'elemento ed ogni classe
	- scelgo la classe per cui la probabilità a posteriori è la maggiore
La classificazione maximum a posteriori tiene conto, al contrario della precedente, di quanti elementi sono presenti in quella determinata classe (discriminante migliore nel momento in cui le likelihood sono molto presenti).
## Classificatore Bayesiano
Fornisce dal punto di vista teorico l'errore minimo possibile tra tutti i classificatori tra i quali possiamo scegliere. Dal punto di vista pratico non è utile, in quanto dipende molto dal processo di apprendimento dei dati. Il classificatore ha la seguente equazione:
$$f_{B}\left(x\right)=\operatorname*{arg~max}_{c\in y}P\left(Y=c\vert X=x\right)$$
Data la probabilità congiunta ci sarebbe possibile calcolare tutte le probabilità del caso (X dato Y, quella di X e quella di Y). Tuttavia, in base alla nostra configurazione dei dati, è possibile che la probabilità congiunta di una configurazione non esista e che il nostro modello interpreti questo come la realtà. 
Questo problema viene chiamato **problema di overfitting**. E' sempre meglio stimare, piuttosto che la congiunta, la likelihood, in maniera tale che almeno una classe sia bloccata. Anche la likelihood è soggetta ad overfitting, ma molto meno. Come?
Ho due possibilità:
- **maximum likelihood in modo parametrico**
	- scelgo una distribuzione di probabilità in base al nostro dataset e ne imparo i parametri per ogni classe:
![](Images/Pasted%20image%2020250310112009.png)
- **maximum likelihood in modo non parametrico**
	- non scelgo la distribuzione ma scegliamo la likelihood in modo non parametrico (ad es. attraverso un istogramma)
Il metodo parametrico una volta calcolati i parametri, i dati sono di poca utilità e i parametri sono pochi, mentre nel metodo non parametrico bisogna memorizzare, ad esempio, tutti gli istogrammi fatti sui dati. Il primo metodo quindi è più efficiente, ma ci blocca ad utilizzare la distribuzione scelta (se i nostri dati finiscono a non avere la distribuzione data inizialmente avremo per forza un errore). Nonostante l'espressività dei metodi parametrici, questi vengono evitati in quanto anch'essi sono soggetti ad overfitting (tanto più grandi quanti sono i valori di *x*).

**Compito per casa:** [slide 11, derivata da 4 per arrivare a 5]
## Risolvere il problema dell'overfitting
Più variabili sono presenti all'interno del dataset, più è possibile che non ci siano configurazioni di un certo tipo. Più variabili ci sono, e più il dataset dovrebbe essere grande, ma ci sono casi in cui questo non è possibile.
Nei casi in cui questo non è possibile, nonostante sia una soluzione molto grossolana, è possibile utilizzare il **naïve density estimator** (ipotizza che le variabili siano **indipendenti** tra loro):
$$P\left(x_{1,}x_{2,}\ldots,x_{k}\vert c\right)=\prod_{i=1}^{k}P\left(x_{i}\vert c\right)=\prod_{i=1}^{k}\Phi_{ci}(xi)$$
#### Training nel caso di naïve Bayes
$$f_{NB}\left(x\right)=\operatorname*{arg~max}_{c\in Y}\prod_{d=1}^{k}\Phi_{cd}\left(x_{d}\right)$$
- imparo una prior per ogni classe
- nel caso della gaussiana, ho bisogno anche di calcolare una media e una deviazione standard per ogni classe
- allo stesso modo, nel caso di una bernoulliana calcolo i parametri di nostro interesse

Supponendo di essere in un problema a due classi, possiamo calcolare per ogni coppia di classe il **decision boundary**, una sorta di confine per cui se passiamo da un lato siamo in una classe. Si calcola eguagliando le likelihood o le posterior, mettendo insieme quindi tutti i punti che hanno uguali valori.

**Es.**
![](Images/Pasted%20image%2020250310113112.png)
L'esempio prende le posterior di due classi con distribuzione gaussiana univariata. Le classi saranno divise da delle parabole ($\sum_{d=1}^{D}\left(a_{d}x_{d}^2+b_{d}x_{d}\right)+c$).
Tale superficie di separazione più è complicata più rischia overfitting, ma in generale è meglio che sia più espressiva.
#### Vantaggi e svantaggi
Da un punto di vista dei vantaggi, il classificatore Bayesiano è quello che si prova subito, in quanto molto veloce e semplice da applicare. Ci dà inoltre un baseline sulla quale confrontare le prestazioni del modello sul nostro dataset.
E' inoltre facilmente interpretabile, essendo che modelliamo ogni singola variabile per conto proprio. Non è molto accurato, ma ci guadagniamo in flessibilità.
## Classificatori lineari
Dal punto di vista teorico, un classificatore è lineare quando input e parametri sono legati da una relazione lineare, mentre dal punto di vista geometrico è una superficie ovviamente lineare. Hanno vantaggi legati alla semplicità del classificatore, ma mancano di espressività.
Supponiamo di avere un problema di classificazione di classi con distribuzione gaussiana multidimensionale. Le classi dovranno avere la stessa matrice di covarianza. Il classificatore Bayesiano diventa quindi:
![](Images/Pasted%20image%2020250311221037.png)
Il nostro LDA (linear discriminant analysis) sarà quindi:
$$f_{LDA}\left(x\right)=\operatorname*{arg~max}_{c\in Y}\Phi_{c}\left(x_{}\right)\pi_{c}$$
Allo stesso modo dei precedenti classificatori, l'LDA viene trainato allo stesso modo:
- Varianza: $\frac{1}{N}\sum_{i=1}^{N}\left(x-\mu\right)^{T}\left(x-\mu\right)$
![](Images/Pasted%20image%2020250311221101.png)
Andando a calcolare il decision boundary:
![](Images/Pasted%20image%2020250311221137.png)
possiamo effettivamente verificare che l'LDA ha un decision boundary lineare, e quindi è un classificatore lineare.
Tale classificatore non fa altro che separare una classe da tutto il resto attraverso una retta. Ci permette di evitare il problema di **masking**.
#### Quadratic discriminant analysis
Nel momento in cui andiamo a rilassare la matrice di covarianza, in un problema con due classi il decision boundary diventa quadratico.
### Vantaggi e svantaggi dell'LDA
E' ovviamente più complicato dell'NB, e questo lo rende un po' più lento. Anch'esso ci offre una buona baseline per confrontare il nostro modello con diverse classificazioni.
Per quanto riguarda i dati, media e varianza di una gaussiana richiedono un dataset più grande rispetto all'NB. Tuttavia, essendo parametrico, è possibile non avere alcune combinazioni di variabili (basta avere un dataset abbastanza grande).
## Modelli discriminativi e generativi
Per adesso abbiamo visto classificatori statistici. Nel momento in cui andiamo a modellare la likelihood, questa offre un prototipo probabilistico che descrive la nostra classe in funzione delle feature che abbiamo. Tale distribuzione sarà possibile quindi campionarla. Questo tipo di approccio si chiama **generativo**: posso chiedere al classificatore non solo se un elemento fa parte di una classe o meno, ma anche di darci degli esempi di elementi.
Nel momento in cui andiamo a calcolare la probabilità a posteriore, non ho la possibilità di campionare, ma solo di richiedere la classe di appartenenza. Tale tipo di modello si chiama **discriminativo**.