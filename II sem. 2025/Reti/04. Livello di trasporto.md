 > *Lezione del 26/03/2025*

Il livello di trasporto estende la consegna di messaggi a due processi applicativi che idealmente seguono comunque un percorso tra due schede di rete. Non abbiamo un solo protocollo di livello di trasporto: esistono infatti sia TCP che UDP. Il loro comune denominatore è l'aggiunta di alcune funzioni:
- concetto di socket
- aggiunta di un'informazione che mi identifica l'endpoint: **numero di porta**
- aggiunta di un meccanismo di controllo dell'errore
Il livello trasporto è implementato sul nodo mittente e destinatario, mentre chiunque è nel mezzo della comunicazione non lo implementa. Tipicamente tali implementazioni sono a livello kernel, mentre i processi applicativi sono a livello *user space*. L'interfaccia del livello trasporto è tipicamente basata sulle *system call* delle API.
#### Servizi di TCP
- **Rende il trasporto affidabile**: i dati che mando arrivano esattamente come vengono spediti
- **Controllo di flusso**: i dati non si perdono nel trasporto e vengono riordinati se arrivano in disordine; regolo il tasso di arrivo dei dati
- **Sistema di acknowledgment**: se perdo dei dati questi vengono ritrasmessi
- **Meccanismo di controllo congestione**

---
 > *Lezione del 28/03/2025*
#### Multiplexing e demultiplexing
Tutti i flussi dati rispetto ad applicazioni diverse si mappano, alla fine, attraverso due schede di rete. Il multiplexing quindi serve essenzialmente per far convogliare diversi flussi di comunicazione in una scheda di rete sola, per poi mandarli ad una certa connessioni di trasporto. Il demultiplexing è l'operazione inversa.
Tutto ciò avviene attraverso un'analisi dell'header a livello trasporto. Dovrò inserire un'informazione tale per far capire all'operazione di demultiplexing come smistare il traffico. Tale informazione è il **numero di porta**: è un numero a 16 bit che sicuramente deve esistere a livello di header di qualsiasi livello di trasporto.
Esistono dei numeri di porta già assegnati in cui ogni numero di porta è associato a servizi già noti, come HTTP (porta 80) o HTTPS. A noi quindi non serve sapere quale porta ci è stata assegnata, quanto più il servizio che stiamo utilizzando.
##### Categorie di porte
- 0-1023: Well Known Ports; porte che secondo RFC non possono essere associati dei servizi. Sono porte privilegiate: bisogna avere il privilegio di amministratore per mettermi in ascolto su tale porta
- 1024-49151: Registered Ports; anche queste sono legate a dei potenziali servizi (la porta riservata a MySequel è la 3306)
- 49152-65535: Dynamic o Private Ports; porte di default utilizzate dal lato client della connessione
Ci possono essere dei casi particolari ovviamente, ma questo è lo scenario tipico.

Quando vediamo quindi uno scambio di dati tra processi diversi, vedremo quindi due ruoli: un client e un server, che solitamente sono due host. Lo scambio dati è bidirezionale: dal client a server e dal server al client. In questo modo avremo una coppia di porte sorgente-destinatario per ogni scambio. Tra 
Implicitamente, se a livello IP i datagram erano identificati da una coppia di IP, adesso avremo una tupla formata da `([IP adddress1, port1], [IP address2, port2])`, che identificano in maniera univoca la comunicazione.
Il numero di porta viene selezionato casualmente, quindi potrebbe essere che alcuni numeri di porta siano effettivamente uguali. Tuttavia, il protocollo non si confonde proprio perché gli IP rimangono diversi.
##### Es.
![](Images/Pasted%20image%2020250409120559.png)

---
# UDP protocol
UDP non aggiunge nulla a livello IP, a parte quello che c'è di minimo sindacale del livello di trasporto. Il suo header sarà quindi molto compatto rispetto a quello del protocollo TCP. Siccome non offre servizi avanzati rispetto al livello IP, questo eredita tutti i pro e contro del protocollo IP:
- lavora in modalità best-effort (se IP sbaglia, UDP si prende l'errore)
- non c'è un concetto di connessione
- è tutto estremamente semplice e veloce (che è la più grossa forza di UDP)
- a livello di latenza quindi paga pochissimo
##### Header
![](Images/Pasted%20image%2020250409181953.png)
- i numeri di porta sono a 16 bit
- il checksum, per motivi prestazionali, si è deciso di calcolarlo solo su informazioni estremamente critiche per il funzionamento del protocollo. Viene calcolato su un cosiddetto **pseudo-header**, che è sostanzialmente una struttura **(che viene costruita al volo e non viene mai trasmessa)** che contiene:
	- indirizzo IP
	- informazioni prese a livello rete
	- numeri di porta del livello UDP
- l'indipendenza tra livelli quindi viene violata. Questa scelta progettuale comporta una serie di problemi: nel momento in cui cambio la struttura dati dell'header IP devo cambiare anche quella del checksum sul livello trasporto. 
- quando il checksum corrisponde, posso assumere con un buon livello di certezza che non ci siano problemi, ma nel momento in cui non lo fa il problema potrebbe essere stato anche nel calcolo del checksum, quindi non c'è sicurezza su chi sia stato a generare l'errore.
- l'operazione di checksum è molto semplice: sommo senza riporto di tutti i dati; se vengono fuori tutti uno siamo a posto, altrimenti no
---
# Reliable channel protocols
Un canale è affidabile solo se capisco quale potrebbero essere errori e minacce, in modo da far resistere il protocollo a questi. TCP quindi rimane affidabile per una serie di cose che sa fare. E' un discorso quindi relativo. **Affidabilità** quindi vuol dire **tolleranza ai guasti**.
Facciamo riferimento ad un modello di trasmissione stop-and-wait, dove mandiamo un pacchetto alla volta. Ci troviamo di fronte a diversi casi:
1. **Messaggio corrotto durante la trasmissione**: ho bisogno di una struttura dati aggiuntiva per identificare l'errore (come ad esempio un checksum o un hash crittografico). Se trovo un errore ritrasmetto; ho bisogno di un messaggio di ACK (acknowledgment) e negative ACK (NAK):
![](Images/Pasted%20image%2020250409183253.png)
2. **Corruzione del messaggio di risposta**: nonostante sia più breve, è un messaggio che comunque transita sulla rete, quindi può essere soggetto a corruzione. Se ricevo un messaggio che non è né ACK né NAK, non ho possibilità di capire se il messaggio sia arrivato corrotto o meno: se ritrasmetto potrei avere un messaggio duplicato, mentre se non lo faccio potrei avere dei dati persi. Ho bisogno di proteggermi quindi rispetto alla duplicazione: è possibile fare ciò aggiungendo un **numero di sequenza**: nel momento in cui mi arriva un messaggio di risposta corrotto, posso reinviare il messaggio; se questo era già arrivato, avrà lo stesso numero di sequenza di un messaggio ricevuto, quindi verrà scartato e verrà mandato di nuovo l'ACK, altrimenti verrà tenuto:
![](Images/Pasted%20image%2020250409183758.png)
- Nel momento in cui immagino di avere diversi pacchetti da inviare contemporaneamente, potrei pensare di sequenziare anche i messaggi di ACK oppure posso implicitamente dire che un NAK lo ottengo mandando un ACK con un numero di sequenza sbagliato (per dire non ho ricevuto N, posso dire che ho ricevuto bene fino ad N-1). 
![](Images/Pasted%20image%2020250409184128.png)
3. **Message loss**: tutto funziona, ma il trigger che fa partire tutto è la ricezione di qualcosa: cosa succede se si perde qualcosa? Nel momento in cui perdo dei dati, fino ad adesso l'ACK non arriverà mai. Ho bisogno di introdurre quindi un **timeout**: quando mando un messaggio faccio partire un timer; se ricevo il messaggio in tempo posso fermare il timer, altrimenti vado in un caso di timeout:
![](Images/Pasted%20image%2020250409184402.png)
Nel livello di trasporto, nell'header, devo mettere sicuramente un numero di sequenza e un numero di sequenza per i messaggi di ACK. Il tempo di timeout infatti non richiede nuove strutture dati.
I modelli stop-and-wait non sono molto efficienti a livello di protocollo, in quanto la rete rimane abbastanza sottoutilizzata: mentre aspetto l'ACK oppure ho un timeout, non faccio niente nel frattempo. Questa cosa esplode se ho dei canali ad alta latenza.

---
# TCP protocol
TCP, a differenza di UDP, è  un protocollo **connection oriented**, quindi ha un'idea di connessione. Riconosco almeno tre fasi: una fase di setup, alla fine della quale mittente e destinatario sono entrambi d'accordo che stanno comunicando, e una fase di chiusura della connessione.
Per certi versi è come una **pipe**: due processi che comunicano non sono necessariamente parenti o sulla stessa macchina, ma hanno comunque un file descriptor dal quale possono leggere e scrivere, facendo arrivare dati in maniera affidabile.
Dal punto di vista del funzionamento non vado a numerare i segmenti, ma i byte del segmento stesso (***byte-stream transmission***): il primo byte darà il numero al segmento stesso (ad esempio, se il segmento 0 è grande 20byte, il segmento successivo avrà numero 20, in quanto il precedente va da 0 a 19).
Al contrario delle pipe però, è **full-duplex**: se ho aperto una connessione tra un server-web e un client-web, il client manderà una richiesta su una connessione TCP e il server risponde sulla stessa connessione. Questo mi permette di avere messaggi in entrambe le direzioni contemporaneamente. 
Infine TCP lavora con un **buffer**: utilizza uno spazio di memoria in cui appoggia temporaneamente dei dati; disaccoppia il comportamento del processo applicativo rispetto alla dinamica di scambio dei dati. Ad esempio, questo permette di compattare diversi dati prima di inviarli al destinatario, e mi permette di riordinarli quando arrivano.
Tutto questo tuttavia mi introduce alcuni limiti:
- TCP non si presta ai ***flussi real-time***
- non ha garanzia di banda
- non funziona in contesti multi-cast, in quanto se uno qualsiasi dei mittenti ha un problema dovrei complicare a dismisura il sistema per la ritrasmissione
TCP nasce più o meno insieme al protocollo UDP, ma viene rimaneggiato nel tempo. Ha la caratteristica di costruire un canale affidabile sopra il livello IP che affidabile non è. Per far questo TCP deve gestire diverse cose:
- ha bisogno di un sistema di sincronizzazione mittente-destinatario
- TCP funziona su reti molto diverse tra loro, quindi ha bisogno di un meccanismo di timeout adattivo
- devo poter gestire pacchetti che tornano dopo un tempo anche molto elevato
- devo poter gestire nodi eterogenei, anche sotto il punto di vista della capacità
- devo poter gestire il congestionamento delle connessioni 
#### Connection oriented
Creo una connessione, la uso e la richiudo. Tutto ciò è trasparente all'utente: se cerco di aprire una connessione e non ci riesco, o se cade mentre la uso, allora ricevo un'errore. 

---
 > *Lezione del 02/04/2025*
##### Header
![](Images/Pasted%20image%2020250402121222.png)
- **HLEN**: lunghezza dell'header
- **Reserved**: utilizzi futuri
- **Code bits**:
	- **ACK**: mi dice se il campo di acknowledgment è utile o meno
	- **PSH**: push, mi dice se il recipiente deve passare i dati immediatamente dopo la ricezione. Non terrà niente nel buffer
	- **URG**: urgent. La prima applicazione del protocollo TCP era uso remoto dei terminale, quindi si aveva necessità di ***informazioni fuori banda***. Tale flag permette di indicare fino a dove i dati sono urgenti (dal byte 0 al byte URG).
	- **SYN, FIN, RST** sono usati per la gestione della connessione
- **Window**: campo che viene settato nel flusso di risposta dal ricevente. Tale finestra indica quanti dati quest0ultimo è ancora disposto ad accettare. Uno degli usi di questo campo è proprio il **controllo di flusso**
E' necessario andare a settare alcune variabili, quali per esempio la ***Maximum Segment Size***. Il campo ***options*** diventa molto importante per questo. Se la MSS è molto piccola, vado a pagare un *overhead* molto grosso: ho un *header* molto corposo rispetto ad un *payload* molto piccolo. Più grande è l'MSS e meglio sto, ma se questo diventa troppo grande, il livello IP potrebbe intasarsi. Per questo l'MSS viene scelta in maniera tale da non causare frammentazione ma essere il più grande possibile.
## Stabilire e chiudere una connessione TCP
Il protocollo TCP nasce con una connessione di tipo ***transfer***. Abbiamo quindi due ruoli distinti:
- un **server** in ascolto su una data porta
- il **client**, che ad un certo punto chiederà al server di aprire una connessione per usufruire dei suoi servizi
Durante la fase di inizializzazione, si crea una connessione per determinare il tipo di buffer da utilizzare, numeri di sequenza ecc. Il discorso dei numeri di sequenza è molto importante per evitare che messaggi di connessioni che ormai sono state chiuse vadano in conflitto con quelli di nuove connessioni.
Quando un client richiede una connessione, manda uno speciale TCP segment, chiamato ***SYN segment*** (SYN per synchronize). All'interno dell'header TCP quindi ci sarà il numero di porta che occuperà nel server (l'indirizzo IP è nel livello inferiore). Altre informazioni che vengono comunicate sono la ***dimensione massima della finestra di ricezione*** ***(MRW)*** e la MSS.
Il server risponderà con un altro segmento SYN:
- ha l'initial sequence number del server
- un messaggio di acknowledgment (flag **ACK** attivo e campo ***acknowledgement*** formato da client_ISN+1)
- dimensione della finestra di ricezione e MSS
Il client dovrà a sua volta comunicare al server che ha accettato la connessione, quindi manderà un messaggio di acknowledge al server.
**SIN -> SIN+ACK -> ACK**
A questo punto client e server comunicano. Una volta fatte tutte le comunicazioni posso decidere di chiudere. Sia il client che il server possono iniziare la fase di chiusura della connessione, anche se solitamente è il client a farlo.
[ minuto 50 ]
Nel caso in cui uno dei due lati ha il "sospetto" che client e server non siano sincronizzati, viene utilizzato il flag RST, che sta per reset, che chiude la connessione immediatamente e rilascia tutte le risorse in uso.
![](Images/Pasted%20image%2020250402125727.png)
![](Images/Pasted%20image%2020250402130433.png)
[ 1h05, prime due slide sulla reliability ]
![](Images/Pasted%20image%2020250402131147.png)
Anche il valore medio non è costante, ma tende a oscillare nel tempo: di notte magari è più basso e meno variabile. Se voglio che questa media si adatti alle condizioni attuali viene utilizzata una formula che è l'***exponential weighted moving average EWMA***:
![](Images/Pasted%20image%2020250402131408.png)
Più $\alpha$ è prossimo ad 1, più la connessione sarà meno stabile ma molto reattiva.
![](Images/Pasted%20image%2020250402131725.png)
Non è statisticamente corretto ma funziona decorosamente e viene utilizzato odiernamente.

---
# Buffer
Il buffer al lato ricevente ha bisogno di un meccanismo di controllo di flusso: lo stop and wait transport mechanism è affidabile e facile da implementare, ma molto inefficiente. Per questo c'è bisogno del buffering per passare da stop-and-wait a pipelining, così da mandare e ricevere pacchetti in maniera simultanea.
### Metriche importanti
- **Round trip time**: tempo che passa tra quando mando pacchetti a quando ricevo la risposta
- **Propagation time**: metà dell'RTT
- **Utilizzazione U**: percentuale di utilizzo delle risorse
- **rate**
- **dimensione del pacchetto L**
- **Tempo di trasmissione dei pacchetti**: Dimensione del pacchetti/rate
- **Tempo di trasmissione effettivo T**: propagazione + trasmissione
#### Stop-and-wait performance
![](Images/Pasted%20image%2020250402132340.png)
#### Pipelining performance
Raddoppio l'utilizazione, ma bisogna che ack sequence number e range of packet siano abbastanza grande. Inoltre, ad ogni momento avrò una certa quantità di dati in transito, quindi avrò una finestra mobile che mi permette di ricevere ACK solo per i pacchetti in transito.

---
# Sliding window
### Sender
Il mittente ha una finestra di pacchetti che ha mandato. Tale finestra ha una certa dimensione: il mittente non può mandare pacchetti oltre questa. Nel momento in cui ricevo un ACK posso traslare in avanti la finestra. Il flusso di avanzamento della finestra quindi dipende dal flusso di invio dei dati da parte del ricevente.
Ho due puntatori, uno per l'ultimo ACK ricevuto e uno per l'ultimo segmento mandato. Tali valori sono vincolati in maniera tale che l'ultimo segmento mandato deve essere all'interno della finestra. Vale quindi la seguente disuguaglianza:
$$LSS-LAR\le SWS$$
![](Images/Pasted%20image%2020250402133455.png)
##### Funzionamento
![](Images/Pasted%20image%2020250402133537.png)
Se l'ACK non viene ricevuto entro il timeout, la finestra non si muove e ritrasmetto il pacchetto puntato da *LSS*
## Recipient
- RWS: dimensione della finestra
- LAS: ultimo segmento accettabile
- LSR: ultimo segmento ricevuto
Vale la stessa disuguaglianza:
$$LAS-LSR \le RSW$$

![](Images/Pasted%20image%2020250402134003.png)
Se ricevo un segmento all'interno della finestra lo metto del buffer si apre la strada a due diversi protocolli:
- go-back-N
- selective retransmission
TCP non usa né uno né l'altro, ma un mix di questi.
---
 > *Lezione del 04/04/2025*
# Go-back-N
L'idea è che quando perdo un segmento semplicemente ricomincio a trasmettere in ordine dal pacchetto perso. Perdere un pacchetto significa che scade il timeout per quel determinato segmento.
![](Images/Pasted%20image%2020250427180058.png)
A lato destinatario posso implementare una logica di acknowledgment cumulativo: quando ricevo l'ACK $x$, significa che ho ricevuto tutti i segmenti in ordine fino al segmento $x$. Se mi arrivano dei segmenti fuori ordine, quindi, non mando l'ACK. Quando mi arriva un segmento fuori ordine, manderò un ACK cumulativo che comprende tutti i segmenti arrivati in ordine e quello che mi appena arrivato.
Nella logica più semplice dell'algoritmo posso addirittura eliminare il buffer di ricezione: se osservo alla lettera tale protocollo, non ho bisogno infatti di tenere fuori ordine; non faccio nessuno sforzo per riordinare i segmenti e mando via tutto ciò che è fuori ordine. 
Sembra inefficiente e di semplice implementazione, ma dal punto di vista implementativo non è così stupido come sembra: la perdita di pacchetti, infatti, è solitamente legata ad uno stato di congestione. Questo vale a dire che la probabilità che si perdano anche i prossimi pacchetti dopo il pacchetto $x$ è molto alta.
![](Images/Pasted%20image%2020250427181158.png)
Funziona principalmente se ho finestre piccole e basse latenze.
# Selective retransmission
Tengo traccia di tutti i segmenti: per ogni segmento mando un ACK diverso; non ho quindi informazioni sui segmenti arrivati in precedenza tramite l'ACK: perdere un pacchetto e perdere un ACK hanno lo stesso valore. Devo inoltre tenere un timer per ogni segmento, che a livello di sistema operativo potrebbe portare ad un overhead abbastanza importante (mantenere molti timer attivi porta ad un gran lavoro da fare). Tuttavia, dal punto di vista dell'overhead della rete andiamo molto meglio, in quanto non ritrasmetto a meno che non abbia perso effettivamente qualcosa: in un contesto in cui la perdita di pacchetti è imputabile ad eventi casuali indipendenti che alla congestione della rete, questo protocollo mi dà un certo vantaggio.
![](Images/Pasted%20image%2020250427181946.png)
Quando scade il timeout quindi, ritrasmetto solo i pacchetti per cui non ho ancora ricevuto un ACK. Assisto a dei movimenti della finestra che sono abbastanza repentini.
Quando ricevo dei segmenti fuori ordini, non faccio avanzare la finestra, ma li tengo, balzando in avanti quando li incontro.
E' un protocollo molto difficile da implementare: non permette ad esempio un'implementazione della finestra con un array statico.
![](Images/Pasted%20image%2020250427182559.png)
![](Images/Pasted%20image%2020250427182622.png)
# TCP algorithm
TCP utilizza un algoritmo che unisce un po' entrambi i mondi: da un lato il meccanismo dell'ACK cumulativo è molto comodo, ma non scarta i pacchetti fuori ordine. Tale approccio viene detto ***di fast recovery***. 
![](Images/Pasted%20image%2020250427183214.png)
- nel secondo caso, non ritrasmettiamo il segmento 100, nonostante arrivi prima l'ACK di 120 e non il suo, proprio perché se arriva l'ACK successivo significa che il pacchetto mandato in precedenza è stato ricevuto correttamente (nel caso 100 non fosse arrivato, infatti, l'ACK da mandare doveva essere il 92).
---
Adesso vorrei evitare che il tasso di invio dei dati vada a saturare il buffer di ricezione e l'evitare di insorgere di congestione sulla rete: si parla di controllo di flusso e controllo di congestione. Alla fine però, nonostante siano due concetti distinti, andranno a lavorare entrambi con il parametro "***window***"
# Flow control
![](Images/Pasted%20image%2020250428094322.png)
- i messaggi di ACK dipendono dai segmenti. Tale ACK dà due informazioni: quanti byte sono stati ricevuti e quanto spazio resta nel buffer di ricezione
- se per caso il destinatario ha riempito il buffer, verrà comunicata una finestra di dimensione zero
- in realtà, quella che io mando è la cosiddetta ***advertised window***, ma per decidere quante cose può ancora mandare, il mittente calcola quella che viene chiamata ***effective window***, che si calcola come ***advertise window*** meno i **segmenti già spediti** e per i quali non ho ancora ricevuto un ACK, in quanto sono segmenti in transito  che prima o poi andranno ad impattare sulla finestra.
Se ho un sistema molto lento a consumare e mando i dati a velocità di rete, vedo come l'advertised e l'effective window si riducono. Quando la seconda arriva a zero, non posso più trasmettere. A livello applicativo, posso ancora accettare dati se ho spazio nei buffer di trasmissione, ma quando anche questo viene riempito, il processo in user space viene sospeso fino a quando non si libera dello spazio nel buffer di trasmissione. Questo vale a dire fino a quando non mando un qualcosa per cui ricevo degli opportuni ACK, che vale a dire infine fino a quando non si sia svuotato un minimo anche il buffer di ricezione. Tutto questo meccanismo fa sì che se il processo consumatore del destinatario è particolarmente lento, arriverò a sospendere l'esecuzione del processo mittente.
Quando arrivo a comunicare una finestra di dimensione zero, tuttavia, potrebbe crearsi una situazione di stallo: 
- il mittente non trasmette più
- se non trasmetto non ricevo ACK
- se non ricevo ACK non riesco ad uscire da questo stato
TCP prevede una contromisura, ossia che anche se la finestra è di dimensione zero, il mittente continuerà a mandare di tanto in tanto un byte randomico, utile per stimolare l'invio di un ACK che mi permette di sapere se la finestra di ricezione si sia liberata. 
Esiste un corollario a tale situazione chiamata ***bad window syndrome***, nella quale entro ed esco dalla situazione di stallo di continuo: la finestra è piena, mando un byte, scopro che c'è dello spazio, mando un segmento e la finestra diventa di nuovo piena. Per evitare tale problema ci sono un paio di contromisure:
- a lato destinatario, faccio finta che il mio buffer sia ancora pieno, fino a che non posso riprendere a ricevere in un modo migliore piuttosto che con una situazione stop and go
- un'altra possibilità è quella di mandare gli ACK in ritardo: questo mi va ad aumentare in maniera forzata l'RTT
C'è un piccolo problema: gli algoritmi che calcolano il timeout in maniera adattiva non funzionano più come dovrebbero. Tuttavia si accetta di avere un timeout sovrastimato in cambio di avere un sistema che cerca di tenerti fuori da tale situazione di stop and go.
L'altra possibilità ovviamente è tenere tale situazione al lato mittente: non mando dati ma solo piccoli probe fino a che non ho una dimensione di finestra comunicata tale da mandare un segmento intero o comunque il buffer destinatario non si sia svuotato a sufficienza.
### Fast retransmission
Fino ad adesso abbiamo detto che possiamo assumere che un pacchetto si sia perso una volta scaduto il timeout. Tuttavia, possono esserci delle altre situazioni che possono infierire in merito. V
- Posso assumere che se ricevo ACK uguali, nonostante abbiamo mandato una serie di segmenti, significa che ancora non è scaduto il timeout per quel segmento e lo sto aspettando. TCP indica però che un triplo ACK duplicato vale come un NAK, assumendo quindi che quel segmento sia stato perso. In un contesto di alta varianza del timeout, questo permette essenzialmente di essere più reattivi (molto importante per evitare che mi accorgo troppo tardi di essere entrato in congestione)
# Congestion control
TCP non ha un meccanismo di notifica della congestione gestito dalla rete. Ci sono dei router che possono settare dei flag dei datagram IP in presenza di congestione, ma non è uno standard. Ho bisogno di un meccanismo di un controllo di congestione quindi che non si basi solo sulle eventuali comunicazioni dei router sullo stato delle proprie code (*controllo end-to-end*).
Quando entro in congestione succedono essenzialmente due cose:
- aumentano i tempi, in quanto i pacchetti passano molto tempo in coda nei router
- le code dei router sono di dimensione finita, quindi ho perdita di pacchetti
L'approccio generale di TCP è un approccio **"slow start"**: parto piano e accelero il tasso di invio dei dati fino a quanto non arrivo ad utilizzare più o meno tutta la banda disponibile. Di solito si parla di ***additive increase-multiplicative decrease***: l'aumento della banda avviene in maniera lineare, e quando sto per entrare in congestione divido la banda passante per un determinato valore.
Il meccanismo di controllo di congestione prevede che la stima del fatto che stiamo entrando in congestione sia fata dal destinatario e si va a comunicare utilizzando la finestra: il campo window quindi viene utilizzato sia per il controllo di flusso che dal controllo di congestione.
In ogni momento, quindi, avremo due valori consigliati per la dimensione della finestra, uno dato dal controllo di flusso e l'altro da quello di congestione, e verrà presa la dimensione **minore**.
Quello che vado a misurare per capire come si sta comportando il sistema è un **throughput** che stimo come la dimensione della mia finestra misurata in MSS, divisa per il RTT:
$$Throughput=\frac{w\cdot MSS}{RTT}$$
- $Throughput$: rateo di trasmissione
- $w$: numero di segmenti nella finestra
- $MSS$: *maximum segment size*
- $RTT$: *round trip time*
Questa è la banda che sta usando la mia connessione, assumendo che il controllo di congestione domina la situazione. Gioco sulla dimensione della finestra per regolare il tasso di invio: se mando messaggi e va tutto bene, non sono in congestione e tale dimensione può essere aumentata; se invece perdo un pacchetto ho bisogno di rallentare e quindi andrò a ridurre, in maniera più o meno drastica, la dimensione della finestra.
Quando lavoro con TCP posso quindi dividere due fasi: una fase dove aumento la finestra (fase stabile) e una dove la riduco (fase di congestione). In più avrò un periodo transitorio iniziale dove parto con una finestra di dimensione unitaria e raddoppio la finestra ad ogni RTT per arrivare ad una situazione regimentale (slow start). Ovviamente, tutte le cose esponenziali tendono a sfuggire di mano, quindi quando raggiungo un determinato valore di soglia non aumento più esponenzialmente ma linearmente, quindi entro nella fase a regime che abbiamo detto inizialmente.
![](Images/Pasted%20image%2020250428161515.png)
### Tahoe e Reno algorithm
Nell'algoritmo **Tahoe** si parte con uno slow start fino al valore di soglia, poi si aumenta linearmente fino a che non si verifica un fenomeno di congestione, che in questo caso viene identificato mediante la perdita di dati in caso di timeout. A questo punto, riporto la finestra a dimensione unitaria e riparto con lo slow start. Tuttavia stavolta aumento linearmente fino ad avvicinarmi ad un valore pari alla metà della dimensione della finestra quando si è verificato l'incidente.
E' un ballo abbastanza brutale in quanto ha forti accelerazioni e brusche frenate.
Più riesco a tenermi con passi piccoli intorno alla zona di equilibrio e più è meglio.
Il primo passo in avanti quindi è stato l'algoritmo **Reno**. Questo considera che posso accorgermi della congestione prima che scada il timeout, utilizzando il **triplo ACK duplicato**. Se scade il timeout si comporta come il precedente, mentre se trova un triplo ACK duplicato dimezza la finestra di congestione e rimane ancora nella fase di crescita lineare.
![](Images/Pasted%20image%2020250428162809.png)
- A parità di tempo, Reno permette di inviare dati in più al destinatario con una banda media più alta del suo contendente
---
 > *Lezione del 09/04/2025*

Questi algoritmi vengono messi particolarmente in crisi dalle reti internet odierni, in quanto la quantità di dati messi in traffico è veramente tanta e crea problemi (***Long Fat Networks, LFN***).
### TCP Vegas
L'elemento chiave è che prima mi accorgo che sto entrando in congestione e meno drastico devo essere nei riguardi delle azioni che devo fare per uscire da questa situazione. E' comunque una situazione in cui la coda è quasi completamente riempita. C'è però un segnale che arriva molto prima rispetto alla perdita di pacchetti e che fino ad adesso abbiamo ignorato: l'aumento della latenza.
Cambio la dimensione della finestra: mi aspetto che questo non abbia effetto sul throughput ma solo sul RTT dei dati. Questo vale a dire che sto per entrare in congestione. In particolare, il cuore di TCP Vegas è la seguente funzione:
![](Images/Pasted%20image%2020250409122050.png)
- quello che mi interessa non è il valore di tale rapporto, quanto il suo segno: se il valore è positivo, vuol dire che la situazione che throughput e windows sono positivamente correlati, quindi decido di prendere delle azioni per ridurre la congestione. Se invece sono negativamente collegati o non ho correlazione, non evidenzio tale comportamento e quindi posso permettermi di aumentare la mia finestra di 1 MSS
Cambia anche il **meccanismo di slow start**: ad ogni RTT, cambio anche la dimensione della finestra.
TCP Vegas è un ottimo sistema di gestione della congestione, ma ha un problema:
- se prendo tutti flussi TCP Vegas, ogni flusso divide egualmente la banda…
- quando ho tanti flussi TCP, flussi diversi possono essere gestiti attraverso algoritmi diversi, e quindi una coesistenza tra flussi TCP Vegas e TCP Reno. TCP Vegas si accorge prima della congestione, quindi rallenta riducendo la sua finestra, mentre Reno la allarga in quanto non c'è stata perdita di pacchetti. Dopo un certo numero di RTT quindi, TCP Vegas è strozzato e TCP Reno incomincia a perdere pacchetti, ma non prima di aver rubato tutta la banda dai flussi TCP Vegas.
# Long Fat Network
- **Banda molto alta**: mi accorgo di cosa sta succedendo dopo un RTT, ma questo avviene dopo che ho già mandato molti altri datagram
- **Latenza molto alta**: introdurre un ritardo si rischia di andare in instabilità è molto alto
Controlli troppo aggressivi quindi causano instabilità, essendo che non siamo molto veloci ad accorgerci di una situazione di congestione. Ci sono quindi alcuni algoritmi proposti per questo specifico caso: TCP Bic e TCP Cubic. Oggi Cubic è lo standard di riferimento, tanto che anche Microsoft lo adotta. 
### TCP Bic
Ha un'idea di qual è il suo waypoint, ossia la stima della banda disponibile; quando sono agli estremi tendo di uscire rapidamente, mentre quando sono nel mezzo tento di fare gli aggiustamenti in maniera più lenta:
![](Images/Pasted%20image%2020250409124406.png)
- Se sono vicino al midpoint, cerco di capire se c'è ancora banda in maniera molto lenta; se c'è ancora spazio continuo in maniera aggressiva. 
Lavoro con quattro finestre:
- finestra corrente
- massimo
- minimo
- valore atteso (prossimo waypoint)
Massimo e minimo sono i valori della ricerca binaria. Se raggiungo il valore atteso, questo diventa il mio valore minimo e cambio target. Se ho un "incidente", vuol dire che devo ridurre la mia finestra massima: il target diventa il vecchio valore di minimo e ricalcolo il minimo, mentre il massimo diventa il valore dove succede l'incidenti.
Lo scopo è sempre quello di danzare vicino al valore atteso; più piccoli saranno gli aggiustamenti e meglio funzionerà il tutto.
TCP Bic riesce ad essere molto consistente anche con una grande presenza di flussi all'interno della rete proprio grazie a questi piccoli aggiustamenti.
### TCP Cubic
Quando ho tanti flussi coesistenti con latenza diversa però finisco di avere dei problemi. Con Bic aggiorno la finestra in base al RTT del flusso; se ne ho uno con latenza bassa e uno con latenza alta, quello con la latenza bassa cambierà la finestra molto più rapidamente e potrebbe prendere prima tutta la banda:
![](Images/Pasted%20image%2020250409125410.png)
Cubic quindi utilizza una funzione cubica: quello che prima facevo con un insieme di spezzate, adesso lo faccio con una funzione cubica percorsa non in base agli RTT ma in base ad un valore calcolato:
[ 100 ]
![](Images/Pasted%20image%2020250409125645.png)
Cubic funziona quindi molto bene in quanto prende tutte le caratteristiche di TCP Bic (non è soggetto ad oscillazioni ed è facilmente scalabile) e aggiunge una *fairness* nei riguardi dei flussi (si comporta ugualmente qualunque RTT abbia il flusso in questione).

---
# Buffer bloating
Perdere pacchetti non è ottimale, quindi metto dei buffer molto grandi per evitarlo. Tuttavia, in questo modo vado a rallentare tutti gli algoritmi di congestione. Chiaramente le LFN creano l'ambiente perfetto per questo tipo di problemi, avendo già banda e latenza elevata e la cosa diventa particolarmente evidente con flussi molto grossi.
C'è una tendenza per la quale più sono capaci le reti più i buffer diventano molto grandi e la situazione può sfuggire di mano. Per prima cosa quindi vengono proposti ***AQM (adaptive queue management)*** algoritmi:
- la dimensione dei buffer vengono gestiti in maniera dinamica
- si presta maggiore attenzione problemi nel momento in cui coesistono diversi flussi TCP
- ***CoDel algorithm*** (controlled delay): introduco perdite che costringono i meccanismi di controllo congestione a ridimensionare la sua finestra e non usano una sola coda
L'altra proposta è che alcuni flussi delle applicazioni rientrano in un solo flusso TCP, evitando di avere tante connessioni difficilmente controllabili.
In verità il CoDel funziona molto bene (nella fase di testing la latenza passa da 1s a 20ms)

Conetto di fairness: se ho N sessioni, ognuna dovrebbe prendersi un n-esimo della banda
![](Images/Pasted%20image%2020250409132034.png)
- Scopo del gioco è dimostrare che dopo un certo numero di turni vorrei tendere al punto d'incontro tra le due bisettrici
- Le finestre vengono aumentate fino a che non entro in congestione. Immaginando di lavorare con TCP Reno, ogni connessione dimezza la sua banda, avvicinandosi sempre di più alla banda.
# UDP vs TCP
Non c'è un vincitore palese, bensì servono per due scopi diversi. Tutte le complessità di TCP si traducono però in possibili overhead: il controllo di congestione parte dall'assunzione che perdita di pacchetti equivale ad avere congestione; questo va bene nella maggior parte degli aspetti, ma va molto male quando hai dei canali radio, i quali possono perdere pacchetti anche per colpa di interferenze elettromagnetiche. TCP è anche molto pesante e ha maggiore latenza, e per quanto riguarda l'impatto sul sistema operativo, devo tenere traccia di molte più strutture dati.
In scenari come server DNS che devono servire molte risoluzioni in pochi secondi, UDP è prezioso perché è leggero: lavora benissimo su reti locali, quando abbiamo applicazioni di tipo richiesta-risposta che stanno in un unico segmento e quando abbiamo scenari per cui l'affidabilità non è così critica (ad esempio, applicazioni multimediali, per le quali non è importanti che tutti i pacchetti arrivino a destinazione).
![](Images/Pasted%20image%2020250409133149.png)
